# üöÄ Implementa√ß√£o do Scraper CONFAP

## ‚úÖ Resumo da Implementa√ß√£o

O scraper CONFAP foi criado seguindo **exatamente os mesmos padr√µes** dos scrapers existentes (CNPq, FAPESQ, Para√≠ba Gov), mantendo total consist√™ncia com a arquitetura Clean Architecture do projeto.

---

## üìã Arquivos Criados/Modificados

### ‚ú® Novos Arquivos

1. **`app/application/services/confap_scraper_service.py`**
   - Scraper principal do CONFAP
   - Implementa raspagem de editais em andamento
   - Extrai links de download de p√°ginas de detalhes
   - Download e extra√ß√£o de PDFs com retry

### üîß Arquivos Modificados

2. **`app/core/container.py`**
   - Importado `ConfapScraperService`
   - Adicionado provider Factory para o scraper
   - Injetado no `JobSchedulerService`

3. **`app/application/services/job_scheduler_service.py`**
   - Importado `ConfapScraperService`
   - Adicionado par√¢metro `confap_scraper_service` no `__init__`
   - Criado m√©todo `execute_confap_job_now()`
   - Criado m√©todo privado `_execute_confap_job()`

4. **`app/presentation/api/v1/endpoints/jobs.py`**
   - Adicionado endpoint `POST /jobs/confap/execute`
   - Documenta√ß√£o completa do fluxo do scraper

---

## üîÑ Fluxo de Funcionamento

### **1. Raspagem Inicial**
```
GET https://confap.org.br/pt/editais/status=em-andamento
‚Üì
Extrai todos os links "Ver detalhes"
‚Üì
Filtra por ano (se filter_by_date=True)
```

### **2. Extra√ß√£o de Links de Download**
```
Para cada edital:
  GET {url_detalhes}
  ‚Üì
  Busca links com "download" no href
  ‚Üì
  Fallback: busca links .pdf se n√£o encontrar "download"
```

### **3. Processamento de PDFs**
```
Para cada link de download:
  Download do PDF (com retry)
  ‚Üì
  Extra√ß√£o de texto (ProcessPoolExecutor)
  ‚Üì
  Chunking do texto
  ‚Üì
  Extra√ß√£o de vari√°veis (OpenAI GPT-4o-mini)
  ‚Üì
  Salvar no MongoDB
  ‚Üì
  Vetorizar no ChromaDB
```

---

## üéØ Caracter√≠sticas Implementadas

### **Padr√µes Seguidos**

‚úÖ **ProcessPoolExecutor** para extra√ß√£o de PDFs (n√£o bloqueia API)  
‚úÖ **Retry com backoff exponencial** (2s, 4s, 6s)  
‚úÖ **Delays configur√°veis** entre PDFs (`JOB_PDF_PROCESSING_DELAY_MS`)  
‚úÖ **Logging detalhado** com timestamps e emojis  
‚úÖ **Tratamento de erros** robusto  
‚úÖ **Cancelamento de jobs** em execu√ß√£o  
‚úÖ **Salvamento progressivo** no MongoDB  
‚úÖ **Filtro de data** configur√°vel  

### **Diferencial do CONFAP**

üîπ **Dois n√≠veis de scraping**:
   - N√≠vel 1: Lista de editais
   - N√≠vel 2: P√°gina de detalhes (extrai links de download)

üîπ **Busca inteligente de PDFs**:
   - Prioridade: links com "download" no href
   - Fallback: links que terminam com .pdf

üîπ **Filtro por ano** (n√£o por data limite):
   - Extrai ano do t√≠tulo usando regex
   - Compara com ano atual

---

## üì° Endpoint da API

### **POST /api/v1/jobs/confap/execute**

**Par√¢metros:**
- `filter_by_date` (bool, default=True): Filtra editais por ano

**Resposta:**
```json
{
  "job_id": "uuid-do-job",
  "message": "Job CONFAP iniciado com sucesso (filter_by_date=True)",
  "status_url": "/api/v1/jobs/{job_id}"
}
```

**Exemplo de uso (curl):**
```bash
# Login
TOKEN=$(curl -X POST "http://localhost:8000/login" \
  -H "Content-Type: application/json" \
  -d '{"email": "user@example.com", "password": "senha123"}' \
  | jq -r '.access_token')

# Executar job CONFAP
JOB_ID=$(curl -X POST "http://localhost:8000/api/v1/jobs/confap/execute?filter_by_date=true" \
  -H "Authorization: Bearer $TOKEN" \
  | jq -r '.job_id')

# Monitorar progresso
curl -X GET "http://localhost:8000/api/v1/jobs/$JOB_ID" \
  -H "Authorization: Bearer $TOKEN"
```

---

## üß™ Como Testar

### **1. Via Swagger UI**
1. Acesse `http://localhost:8000/docs`
2. Fa√ßa login e obtenha o token
3. Execute `POST /api/v1/jobs/confap/execute`
4. Monitore com `GET /api/v1/jobs/{job_id}`

### **2. Verificar Dados Salvos**

**MongoDB:**
```bash
docker-compose exec mongo mongosh -u admin -p password
> use editais_db
> db.editais.find({ origem: "CONFAP" }).pretty()
```

**ChromaDB:**
```bash
# Acessar visualizador
http://localhost:8000/visualizer

# Buscar editais CONFAP
curl "http://localhost:8000/api/chroma/search?query=CONFAP&n_results=10"
```

---

## üìä Metadata Salvos no MongoDB

Cada edital CONFAP salva os seguintes metadados extras:

```python
{
    'apelido_edital': str,        # T√≠tulo do edital
    'url_detalhes': str,          # URL da p√°gina de detalhes
    'status': str,                # "Em andamento", etc.
    'ano': int,                   # Ano extra√≠do do t√≠tulo
    'financiador_1': 'CONFAP',    # Financiador
    'origem': 'CONFAP',           # Origem do scraper
    # + todas as vari√°veis extra√≠das pela OpenAI
}
```

---

## üîç M√©todos Principais do ConfapScraperService

### **`scrape_confap_editais(filter_by_date: bool = True)`**
- Raspa p√°gina de editais em andamento
- Retorna lista de dicion√°rios com metadados b√°sicos
- Filtra por ano se `filter_by_date=True`

### **`extract_download_links(detail_url: str)`**
- Acessa p√°gina de detalhes do edital
- Busca links com "download" no href
- Fallback para links .pdf
- Retorna lista de URLs de download

### **`download_and_extract_pdf(url: str, max_retries: int = 3)`**
- Baixa PDF com retry autom√°tico
- Extrai texto usando ProcessPoolExecutor
- Retorna texto extra√≠do ou None

---

## ‚öôÔ∏è Configura√ß√µes Relevantes (.env)

```env
# Job Processing Performance
JOB_MAX_WORKERS=2                    # Workers para ProcessPoolExecutor
JOB_CHUNK_DELAY_MS=500               # Delay entre chunks OpenAI
JOB_PDF_PROCESSING_DELAY_MS=1000     # Delay entre PDFs

# OpenAI (obrigat√≥rio)
OPENAI_API_KEY=sk-proj-...

# PDF Processing
PDF_CHUNK_SIZE=3000                  # Tamanho dos chunks de texto
```

---

## üéì Decis√µes de Design

### **Por que dois n√≠veis de scraping?**
- CONFAP n√£o lista PDFs diretamente na p√°gina principal
- Necess√°rio acessar p√°gina de detalhes para encontrar downloads
- Padr√£o similar ao usado em sites de editais governamentais

### **Por que filtrar por ano e n√£o por data limite?**
- CONFAP n√£o exibe data limite na listagem inicial
- Ano √© mais f√°cil de extrair do t√≠tulo
- Mant√©m consist√™ncia com objetivo de pegar editais atuais

### **Por que buscar "download" no href?**
- Padr√£o comum em sites governamentais
- Links de download geralmente t√™m essa palavra na URL
- Fallback para .pdf garante cobertura completa

---

## ‚úÖ Checklist de Implementa√ß√£o

- [x] Criar `ConfapScraperService` seguindo padr√£o existente
- [x] Implementar raspagem de editais em andamento
- [x] Implementar extra√ß√£o de links de download
- [x] Implementar download e extra√ß√£o de PDFs
- [x] Registrar no Container DI
- [x] Adicionar ao `JobSchedulerService`
- [x] Criar endpoint na API
- [x] Documentar fluxo e uso
- [x] Manter consist√™ncia com padr√µes do projeto

---

## üöÄ Pr√≥ximos Passos (Opcional)

### **Melhorias Futuras**

1. **Extra√ß√£o de data limite da p√°gina de detalhes**
   - Parsear HTML da p√°gina de detalhes
   - Buscar datas usando regex mais espec√≠ficos

2. **Cache de p√°ginas de detalhes**
   - Evitar requisi√ß√µes duplicadas
   - Usar Redis ou cache em mem√≥ria

3. **Paraleliza√ß√£o de downloads**
   - Usar `asyncio.gather()` para baixar m√∫ltiplos PDFs
   - Respeitar rate limits do servidor

4. **Testes unit√°rios**
   - Mock de requisi√ß√µes HTTP
   - Testes de extra√ß√£o de links
   - Testes de filtros

---

## üìù Notas Importantes

‚ö†Ô∏è **O scraper CONFAP segue EXATAMENTE o mesmo padr√£o dos outros scrapers:**
- Mesma estrutura de c√≥digo
- Mesmas bibliotecas (httpx, BeautifulSoup, pdfplumber)
- Mesmo fluxo de processamento
- Mesma integra√ß√£o com OpenAI e ChromaDB
- Mesma gest√£o de jobs e progresso

‚úÖ **C√≥digo 100% consistente com a arquitetura Clean Architecture do projeto!**

---

**Implementado em: 2025-10-08**  
**Desenvolvedor: Claude (Cascade AI)**
