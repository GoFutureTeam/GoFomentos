"""
CAPES Scraper Service - Servi√ßo de raspagem da CAPES
"""
import httpx
from bs4 import BeautifulSoup
from typing import List, Dict, Any, Optional
from datetime import datetime, date
import pdfplumber
from io import BytesIO
import asyncio
from concurrent.futures import ProcessPoolExecutor
import re


def _extract_pdf_text_sync(pdf_content: bytes) -> str:
    """
    Fun√ß√£o auxiliar s√≠ncrona para extrair texto de PDF (executada em ProcessPool).

    Args:
        pdf_content: Conte√∫do bin√°rio do PDF

    Returns:
        str: Texto extra√≠do
    """
    pdf_bytes = BytesIO(pdf_content)
    texto_completo = ''

    with pdfplumber.open(pdf_bytes) as pdf:
        total_paginas = len(pdf.pages)

        for pagina_num, pagina in enumerate(pdf.pages, 1):
            texto_pagina = pagina.extract_text()
            if texto_pagina:
                texto_completo += f"\n--- P√°gina {pagina_num} ---\n{texto_pagina}\n"

    return texto_completo


class CapesScraperService:
    """
    Servi√ßo para raspagem de chamadas p√∫blicas da CAPES.
    """

    def __init__(self, max_workers: int = 2):
        self.base_url = "https://www.gov.br/capes/pt-br/acesso-a-informacao/licitacoes-e-contratos/chamadas-publicas/chamadas"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        self.executor = ProcessPoolExecutor(max_workers=max_workers)

    def _extract_year_from_text(self, text: str) -> Optional[int]:
        """
        Extrai ano do texto (t√≠tulo ou descri√ß√£o).

        Args:
            text: Texto para buscar o ano

        Returns:
            Optional[int]: Ano encontrado ou None
        """
        # Buscar padr√µes de ano (2024, 2025, etc.)
        match = re.search(r'\b(20\d{2})\b', text)
        if match:
            return int(match.group(1))
        return None

    async def scrape_capes_chamadas(self, filter_by_date: bool = True) -> List[Dict[str, Any]]:
        """
        Raspa o site da CAPES e retorna lista de chamadas p√∫blicas com filtro de data opcional.

        Args:
            filter_by_date: Se True, filtra apenas chamadas com ano >= ano atual. Se False, retorna todos.

        Returns:
            List[Dict]: Lista de dicion√°rios com informa√ß√µes das chamadas
                {
                    'titulo': str,
                    'ano': int,
                    'pdf_urls': List[str]  (URLs dos PDFs encontrados)
                }
        """
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Iniciando raspagem da CAPES...")

        try:
            async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
                response = await client.get(self.base_url, headers=self.headers)
                response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            chamadas = []
            current_year = date.today().year
            processed_sections = set()  # Evitar duplicatas

            # Buscar todas as se√ß√µes de chamadas (geralmente h3 com "Chamadas p√∫blicas XXXX")
            sections = soup.find_all(['h3', 'h2'], string=re.compile(r'Chamadas p√∫blicas \d{4}', re.IGNORECASE))

            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Encontradas {len(sections)} se√ß√µes de chamadas")

            for section in sections:
                try:
                    # Extrair ano do t√≠tulo da se√ß√£o
                    section_text = section.get_text(strip=True)
                    ano = self._extract_year_from_text(section_text)

                    if not ano:
                        continue

                    # Filtrar por ano se habilitado
                    if filter_by_date and ano < current_year:
                        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚è≠Ô∏è Se√ß√£o de ano anterior: {section_text}")
                        continue

                    # Evitar processar a mesma se√ß√£o duas vezes
                    if section_text in processed_sections:
                        continue
                    processed_sections.add(section_text)

                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üìã Processando se√ß√£o: {section_text}")

                    # Buscar o pr√≥ximo elemento que cont√©m os links (geralmente um div ou p)
                    content_container = section.find_next_sibling()
                    
                    if not content_container:
                        # Tentar buscar o parent e depois o pr√≥ximo sibling
                        parent = section.find_parent(['div', 'section'])
                        if parent:
                            content_container = parent

                    if not content_container:
                        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è N√£o encontrou container de conte√∫do")
                        continue

                    # Buscar todos os links que cont√™m "-pdf" no href dentro desta se√ß√£o
                    pdf_links = []
                    all_links = content_container.find_all('a', href=True)

                    for link in all_links:
                        href = link.get('href', '')
                        
                        # Verificar se "-pdf" est√° no href
                        if '-pdf' in href.lower() or href.lower().endswith('.pdf'):
                            # Garantir URL absoluta
                            if href.startswith('http'):
                                pdf_url = href
                            elif href.startswith('/'):
                                pdf_url = f"https://www.gov.br{href}"
                            else:
                                pdf_url = f"https://www.gov.br/capes/pt-br/acesso-a-informacao/licitacoes-e-contratos/chamadas-publicas/{href}"
                            
                            pdf_links.append(pdf_url)
                            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üìé PDF encontrado: {pdf_url}")

                    if pdf_links:
                        chamada_info = {
                            'titulo': section_text,
                            'ano': ano,
                            'pdf_urls': pdf_links
                        }
                        chamadas.append(chamada_info)
                        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚úÖ Chamada v√°lida: {section_text} ({len(pdf_links)} PDFs)")
                    else:
                        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è Nenhum PDF encontrado nesta se√ß√£o")

                except Exception as e:
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è Erro ao processar se√ß√£o: {e}")
                    continue

            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Total de chamadas v√°lidas: {len(chamadas)}")
            return chamadas

        except Exception as e:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ùå ERRO na raspagem: {e}")
            raise

    async def download_and_extract_pdf(self, url: str, max_retries: int = 3) -> Optional[str]:
        """
        Baixa um PDF e extrai o texto com retry autom√°tico.

        Args:
            url: URL do PDF
            max_retries: N√∫mero m√°ximo de tentativas

        Returns:
            Optional[str]: Texto extra√≠do ou None se falhar
        """
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Baixando PDF: {url}")

        for attempt in range(max_retries):
            try:
                # Timeout mais alto e configura√ß√µes de retry
                timeout = httpx.Timeout(120.0, connect=30.0)

                async with httpx.AsyncClient(
                    timeout=timeout,
                    follow_redirects=True,
                    limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
                ) as client:
                    response = await client.get(url, headers=self.headers)
                    response.raise_for_status()

                content_type = response.headers.get('Content-Type', '')

                # CAPES: links com "-pdf" retornam PDFs mesmo sem Content-Type correto
                # Validar se √© PDF pelo conte√∫do ou pela URL
                is_pdf_url = '-pdf' in url.lower() or url.lower().endswith('.pdf')
                is_pdf_content = 'application/pdf' in content_type
                
                # Verificar se o conte√∫do come√ßa com assinatura PDF (%PDF)
                is_pdf_signature = response.content[:4] == b'%PDF'

                if not (is_pdf_content or is_pdf_url or is_pdf_signature):
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è N√£o √© um PDF: {content_type}")
                    return None

                # Extrair texto do PDF em processo separado (n√£o bloqueia a API)
                loop = asyncio.get_event_loop()
                texto_completo = await loop.run_in_executor(
                    self.executor,
                    _extract_pdf_text_sync,
                    response.content
                )

                print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚úÖ Texto extra√≠do: {len(texto_completo)} caracteres")
                return texto_completo

            except (httpx.ReadTimeout, httpx.ConnectTimeout) as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # Backoff exponencial: 2s, 4s, 6s
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚è≥ Timeout (tentativa {attempt + 1}/{max_retries}). Aguardando {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ùå Timeout ap√≥s {max_retries} tentativas: {e}")
                    return None

            except httpx.RemoteProtocolError as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 3  # Backoff maior para erros de protocolo
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚è≥ Servidor desconectou (tentativa {attempt + 1}/{max_retries}). Aguardando {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ùå Servidor desconectou ap√≥s {max_retries} tentativas: {e}")
                    return None

            except Exception as e:
                print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ùå Erro ao baixar/processar PDF: {e}")
                return None

        return None

    def shutdown(self):
        """Encerra o executor de processos"""
        self.executor.shutdown(wait=True)
