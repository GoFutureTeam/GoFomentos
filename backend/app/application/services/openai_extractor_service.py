"""
OpenAI Extractor Service - Extra√ß√£o de vari√°veis com LLM
"""
from typing import Dict, Any, List, Optional
from datetime import datetime
import json
import re
import asyncio
from openai import AsyncOpenAI

from ...domain.repositories.edital_repository import EditalRepository


class OpenAIExtractorService:
    """
    Servi√ßo para extra√ß√£o de vari√°veis de editais usando OpenAI.
    Implementa salvamento progressivo no MongoDB e vetoriza√ß√£o no ChromaDB.
    """

    def __init__(
        self,
        openai_api_key: str,
        edital_repository: EditalRepository,
        chromadb_service: Optional[Any] = None,
        chunk_delay_ms: int = 500
    ):
        """
        Inicializa o servi√ßo.

        Args:
            openai_api_key: Chave da API OpenAI
            edital_repository: Reposit√≥rio de editais
            chromadb_service: Servi√ßo ChromaDB (opcional)
            chunk_delay_ms: Delay em milissegundos entre chunks para n√£o sobrecarregar a API
        """
        self.client = AsyncOpenAI(api_key=openai_api_key)
        self.edital_repo = edital_repository
        self.chromadb_service = chromadb_service
        self.chunk_delay_ms = chunk_delay_ms

    def _chunk_text(self, text: str, chunk_size: int = 1500, overlap_sentences: int = 3) -> List[str]:
        """
        Divide o texto em chunks sem√¢nticos, preservando contexto e estrutura.

        ESTRAT√âGIA H√çBRIDA:
        1. Detecta se√ß√µes maiores (CRONOGRAMA, ELEGIBILIDADE, etc.)
        2. Quebra por par√°grafos duplos (estrutura comum em editais)
        3. Agrupa at√© atingir chunk_size sem quebrar par√°grafos
        4. Overlap sem√¢ntico: repete √∫ltimas N senten√ßas do chunk anterior

        Args:
            text: Texto completo
            chunk_size: Tamanho alvo de cada chunk (caracteres)
            overlap_sentences: N√∫mero de senten√ßas a repetir entre chunks

        Returns:
            List[str]: Lista de chunks sem√¢nticos
        """
        # 1. Normalizar quebras de linha
        text = re.sub(r'\n{3,}', '\n\n', text)  # M√∫ltiplas quebras -> dupla

        # 2. Identificar blocos naturais (par√°grafos ou se√ß√µes)
        # Tenta detectar se√ß√µes com t√≠tulos em CAPS ou numerados
        section_pattern = r'\n\n(?=[A-Z√á√É√ï\d][A-Z√á√É√ï\s\d\-:.]{5,}(?:\n|$))'
        sections = re.split(section_pattern, text)

        chunks = []
        current_chunk = ""
        previous_sentences = []

        for section in sections:
            # Quebrar se√ß√£o em par√°grafos
            paragraphs = section.split('\n\n')

            for paragraph in paragraphs:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue

                # Se adicionar este par√°grafo ultrapassa o limite
                if len(current_chunk) + len(paragraph) > chunk_size and current_chunk:
                    # Salvar chunk atual
                    chunks.append(current_chunk.strip())

                    # Extrair √∫ltimas N senten√ßas para overlap
                    sentences = re.split(r'(?<=[.!?])\s+', current_chunk)
                    previous_sentences = sentences[-overlap_sentences:] if len(sentences) > overlap_sentences else sentences

                    # Iniciar novo chunk com overlap
                    current_chunk = ' '.join(previous_sentences) + '\n\n' + paragraph
                else:
                    # Adicionar par√°grafo ao chunk atual
                    if current_chunk:
                        current_chunk += '\n\n' + paragraph
                    else:
                        current_chunk = paragraph

        # Adicionar √∫ltimo chunk
        if current_chunk.strip():
            chunks.append(current_chunk.strip())

        # 3. P√≥s-processamento: chunks muito pequenos ou muito grandes
        final_chunks = []
        for chunk in chunks:
            # Se chunk muito pequeno (<500 chars), tentar mesclar com anterior
            if len(chunk) < 500 and final_chunks:
                final_chunks[-1] += '\n\n' + chunk
            # Se chunk muito grande (>3000 chars), for√ßar quebra por senten√ßas
            elif len(chunk) > 3000:
                sentences = re.split(r'(?<=[.!?])\s+', chunk)
                temp_chunk = ""
                for sent in sentences:
                    if len(temp_chunk) + len(sent) > chunk_size and temp_chunk:
                        final_chunks.append(temp_chunk.strip())
                        temp_chunk = sent
                    else:
                        temp_chunk += ' ' + sent if temp_chunk else sent
                if temp_chunk.strip():
                    final_chunks.append(temp_chunk.strip())
            else:
                final_chunks.append(chunk)

        return final_chunks if final_chunks else [text]

    def _merge_variables(self, accumulated: Dict, new: Dict) -> Dict:
        """
        Merge inteligente de vari√°veis extra√≠das.
        Prioriza valores mais completos e n√£o-nulos.

        Args:
            accumulated: Dicion√°rio acumulado
            new: Novo dicion√°rio com vari√°veis

        Returns:
            Dict: Dicion√°rio merged
        """
        for key, value in new.items():
            # Nunca sobrescrever link e uuid que v√™m do sistema
            if key in ["link", "uuid"]:
                continue

            if value is not None and value != "":
                # Se o campo ainda n√£o existe ou √© nulo, adiciona
                if accumulated.get(key) is None or accumulated.get(key) == "":
                    accumulated[key] = value
                # Se ambos t√™m valor string, mant√©m o mais longo
                elif isinstance(value, str) and isinstance(accumulated.get(key), str):
                    if len(value) > len(accumulated[key]):
                        accumulated[key] = value
                # Para n√∫meros, mant√©m o n√£o-zero
                elif isinstance(value, (int, float)) and value != 0:
                    if accumulated.get(key) == 0 or accumulated.get(key) is None:
                        accumulated[key] = value

        return accumulated

    async def extract_variables_progressive(
        self,
        text: str,
        edital_uuid: str,
        pdf_url: str,
        max_retries: int = 1
    ) -> Dict[str, Any]:
        """
        Extrai vari√°veis do texto chunk por chunk e SALVA PROGRESSIVAMENTE.

        Args:
            text: Texto completo do edital
            edital_uuid: UUID do edital
            pdf_url: URL do PDF
            max_retries: N√∫mero m√°ximo de tentativas em caso de erro

        Returns:
            Dict[str, Any]: Vari√°veis consolidadas
        """
        chunks = self._chunk_text(text)
        accumulated_vars = {
            "link": pdf_url,
            "uuid": edital_uuid
        }

        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üìä Total de chunks: {len(chunks)}")

        for i, chunk in enumerate(chunks, 1):
            retry_count = 0
            success = False

            while retry_count <= max_retries and not success:
                try:
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üîÑ Processando chunk {i}/{len(chunks)}")

                    # Extrair vari√°veis do chunk
                    chunk_vars = await self._extract_chunk(chunk, i, len(chunks))

                    # ‚úÖ SALVAR NO MONGODB A CADA CHUNK
                    await self.edital_repo.save_partial_extraction(
                        edital_uuid=edital_uuid,
                        chunk_index=i,
                        variables=chunk_vars,
                        status="in_progress"
                    )
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üíæ Chunk {i} salvo no MongoDB")

                    # üîç VETORIZAR E SALVAR NO CHROMADB
                    if self.chromadb_service:
                        try:
                            edital_name = chunk_vars.get('apelido_edital') or accumulated_vars.get('apelido_edital') or 'Edital CNPq'
                            await self.chromadb_service.add_chunk(
                                chunk_text=chunk,
                                edital_uuid=edital_uuid,
                                edital_name=edital_name,
                                chunk_index=i,
                                total_chunks=len(chunks),
                                metadata={
                                    "financiador": chunk_vars.get('financiador_1') or chunk_vars.get('financiador_2'),
                                    "area_foco": chunk_vars.get('area_foco'),
                                    "link": pdf_url
                                }
                            )
                        except Exception as e:
                            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è Erro ao vetorizar chunk {i} no ChromaDB: {e}")

                    # Merge com vari√°veis acumuladas
                    accumulated_vars = self._merge_variables(accumulated_vars, chunk_vars)

                    success = True

                    # ‚è±Ô∏è Delay entre chunks para n√£o sobrecarregar a event loop da API
                    await asyncio.sleep(self.chunk_delay_ms / 1000.0)

                except Exception as e:
                    retry_count += 1
                    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ùå Erro no chunk {i} (tentativa {retry_count}/{max_retries + 1}): {e}")

                    if retry_count > max_retries:
                        # Registrar erro mas continuar
                        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è Pulando chunk {i} ap√≥s {max_retries + 1} tentativas")
                        break

        # ‚úÖ GARANTIR QUE LINK E UUID ESTEJAM PRESENTES
        accumulated_vars["link"] = pdf_url
        accumulated_vars["uuid"] = edital_uuid

        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üîó DEBUG: Link do PDF antes de salvar: '{pdf_url}'")
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] üîó DEBUG: Link em accumulated_vars: '{accumulated_vars.get('link')}'")

        # ‚úÖ SALVAR CONSOLIDADO FINAL NO MONGODB
        await self.edital_repo.save_final_extraction(
            edital_uuid=edital_uuid,
            consolidated_variables=accumulated_vars,
            status="completed"
        )
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚úÖ Vari√°veis consolidadas salvas no MongoDB")

        return accumulated_vars

    async def _extract_chunk(self, chunk: str, chunk_index: int, total_chunks: int) -> Dict[str, Any]:
        """
        Extrai vari√°veis de um √∫nico chunk usando OpenAI.

        Args:
            chunk: Texto do chunk
            chunk_index: √çndice do chunk atual
            total_chunks: Total de chunks

        Returns:
            Dict[str, Any]: Vari√°veis extra√≠das
        """
        prompt = f"""
Voc√™ √© um especialista em an√°lise de editais de fomento √† pesquisa e inova√ß√£o no Brasil.
Sua tarefa √© extrair informa√ß√µes estruturadas de editais de ag√™ncias como CNPq, FAPESQ, FINEP, CONFAP, CAPES, etc.

INSTRU√á√ïES IMPORTANTES:
1. Leia TODO o texto cuidadosamente antes de extrair
2. Se um campo n√£o estiver expl√≠cito no texto, preencha com null (n√£o com string vazia)
3. Para datas, use o formato YYYY-MM-DD
4. Para valores monet√°rios, extraia apenas o n√∫mero (sem R$, pontos ou v√≠rgulas)
5. Para percentuais, extraia apenas o n√∫mero (sem % ou s√≠mbolos)
6. Para dura√ß√µes, converta para MESES (ex: "12 meses", "1 ano" = 12 meses)
7. Para booleanos, use true/false (n√£o strings)

CAMPOS A EXTRAIR:

{{
  "apelido_edital": "T√≠tulo/nome completo do edital (ex: 'Chamada P√∫blica FAPESQ N¬∫ 01/2025')",
  "financiador_1": "Institui√ß√£o principal que financia (ex: 'CNPq', 'FAPESQ-PB', 'FINEP', 'CONFAP', 'CAPES')",
  "financiador_2": "Institui√ß√£o secund√°ria ou parceira (null se n√£o houver)",
  "area_foco": "√Årea(s) tem√°tica(s) do edital (ex: 'Sa√∫de', 'Tecnologia', 'Mudan√ßas Clim√°ticas')",
  "tipo_proponente": "Quem pode se candidatar (ex: 'Pesquisadores doutores', 'Institui√ß√µes de Ensino', 'Empresas')",
  "empresas_que_podem_submeter": "Tipos espec√≠ficos de empresas eleg√≠veis (ex: 'PMEs', 'Startups', 'Empresas brasileiras')",
  "duracao_min_meses": "Dura√ß√£o m√≠nima do projeto EM MESES (n√∫mero inteiro ou null)",
  "duracao_max_meses": "Dura√ß√£o m√°xima do projeto EM MESES (n√∫mero inteiro ou null)",
  "valor_min_R$": "Valor m√≠nimo de financiamento em REAIS (n√∫mero ou null)",
  "valor_max_R$": "Valor m√°ximo de financiamento em REAIS (n√∫mero ou null)",
  "tipo_recurso": "Tipo de recurso oferecido (ex: 'Bolsas', 'Financiamento n√£o-reembols√°vel', 'Subven√ß√£o econ√¥mica')",
  "recepcao_recursos": "Como os recursos ser√£o recebidos (ex: 'Diretamente ao pesquisador', 'Via institui√ß√£o')",
  "custeio": "Permite gastos de custeio? (true/false/null)",
  "capital": "Permite gastos de capital (equipamentos)? (true/false/null)",
  "contrapartida_min_%": "Percentual m√≠nimo de contrapartida exigida (n√∫mero ou null)",
  "contrapartida_max_%": "Percentual m√°ximo de contrapartida exigida (n√∫mero ou null)",
  "tipo_contrapartida": "Tipo de contrapartida aceita (ex: 'Financeira', 'Econ√¥mica', 'N√£o h√°')",
  "data_inicial_submissao": "Data de ABERTURA das submiss√µes (YYYY-MM-DD ou null)",
  "data_final_submissao": "Data de ENCERRAMENTO/PRAZO das submiss√µes (YYYY-MM-DD ou null)",
  "data_resultado": "Data prevista para divulga√ß√£o dos RESULTADOS (YYYY-MM-DD ou null)",
  "descricao_completa": "Resumo do objetivo/finalidade do edital em 1-2 frases",
  "origem": "Ag√™ncia de origem (extraia do texto: 'CNPq', 'FAPESQ', 'FINEP', 'CONFAP', 'CAPES', 'Governo da Para√≠ba', etc.)",
  "observacoes": "Observa√ß√µes importantes, requisitos especiais ou restri√ß√µes mencionadas"
}}

IMPORTANTE:
- Este √© o chunk {chunk_index} de {total_chunks}. Alguns campos podem estar em outros chunks.
- Retorne APENAS o JSON v√°lido, sem markdown, coment√°rios ou texto adicional.
- Use null para campos ausentes, N√ÉO use string vazia "" ou "null".

Texto do edital:
---
{chunk}
---

JSON extra√≠do:"""

        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )

        resposta_llm = response.choices[0].message.content.strip()

        # Limpar markdown code blocks
        if "```json" in resposta_llm:
            match = re.search(r'```json\s*(.*?)\s*```', resposta_llm, re.DOTALL)
            if match:
                resposta_llm = match.group(1).strip()
        elif "```" in resposta_llm:
            resposta_llm = resposta_llm.replace("```", "").strip()

        # Parse JSON
        try:
            variables = json.loads(resposta_llm)

            # Converter strings "null" em None
            for key, value in variables.items():
                if isinstance(value, str) and value.lower() == "null":
                    variables[key] = None

            return variables
        except json.JSONDecodeError as e:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ‚ö†Ô∏è Resposta n√£o √© JSON v√°lido: {e}")
            return {"erro": "resposta_invalida", "raw": resposta_llm[:500]}
